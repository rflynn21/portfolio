[
  {
    "path": "posts/2021-01-03-whose-interceptions-cost-their-team-the-most-tutorial/",
    "title": "Whose Interceptions Cost Their Team the Most? (Tutorial)",
    "description": "In this post, I thought I would give an introduction into (nflfastR) and its potential applications through a simple example in R.",
    "author": [
      {
        "name": "Ryan Flynn",
        "url": "https://ryanflynn.netlify.app/about.html"
      }
    ],
    "date": "2021-01-03",
    "categories": [],
    "contents": "\r\nIt goes without saying that in the NFL, all interceptions are not created equal. Some interceptions cost their teams much more in terms of their chances of winning the game. For example, rookie Cincinnati Bengals QB Joe Burrow threw an interception in week 6 against the Indianapolis Colts as the Bengals were down by 4 at the 35-yard line with just 46 seconds left, and a chance to go score to win the game. This interception sealed the fate of the Bengals, and cost the team much more than if the interception had come, say, in the 1st quarter. Using the phenomenal nflfastR package, we can quantify just how much these interceptions cost, and which quarterbacks have cost their teams the most overall with their interceptions.\r\nIf you haven’t already, go ahead and install, then load the nflfastR package using the functions:\r\n\r\n\r\n# install.packages('nflfastR')\r\nlibrary(nflfastR)\r\n\r\n\r\n\r\nAdditionally, my code is generally pretty reliant on tidyverse, so let’s load that as well:\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nNext, we need to load in the play-by-play data, which is what makes nflfastR so great. There are ways to read in the data using the nflfastR package itself, but the faster way of doing it is by reading in the RDS files straight from the GitHub repository. In this case, we are reading in the 2020 play-by-play data, but you can easily read any other available year by changing the 2020 at the end of the string to whichever year you want.\r\n\r\n\r\npbp <- readRDS(url('https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_2020.rds'))\r\n\r\n\r\n\r\nIn these few lines, we are reading in the entire 2020 NFL schedule, but we want to filter out any games that haven’t been played yet:\r\n\r\n\r\nschedule <- fast_scraper_schedules(2020)\r\ncompleted_games <- schedule %>%\r\n  filter(!is.na(home_result))\r\n\r\n\r\n\r\nNext, it may not make sense just yet, but we will come back to it. These lines create a dataframe called colors that contains the color codes for every team so we can make our plots prettier.\r\n\r\n\r\ncolors <- teams_colors_logos\r\nDF = data.frame(table(unlist(completed_games)))\r\n\r\ncolors <- colors %>%\r\n  left_join(DF, by = c(\"team_abbr\" = \"Var1\")) %>%\r\n  rename(\"games_played\" = \"Freq\")\r\n\r\n\r\n\r\nOkay - this may look like a lot, so let’s walk through it. We are making a new dataframe, based on our pbp dataframe, called ints. - The first line is a filter since we only care about plays that were interceptiions. - This second line is a housekeeping line. The pbp data frame has 340 columns worth of data. We only really care about a few, so let’s remove all the stuff we don’t need, so it’s easier to see. - This line groups all the rows by whoever the passer is, meaning, for example, any time passer_player_name == “Baker Mayfield”, it all gets counted as one single group. - This line uses the row_number() function to count each interception. Mayfield’s third interception of the season is listed as 3 under the column n - Lastly, we are joining the colors data frame to this new ints data frame on the basis of the team. So since any of Mayfield’s plays will have “CLE” listed as the ‘posteam’ (possessing team), this will match with wherever “CLE” is listed in the team_abbr column of colors.\r\n\r\n\r\nints <- pbp %>%\r\n  filter(interception == 1) %>%\r\n  select(posteam, passer_player_name, wpa, week, defteam) %>%\r\n  group_by(passer_player_name) %>%\r\n  mutate(n = row_number()) %>%\r\n  left_join(colors, by = c(\"posteam\" = \"team_abbr\"))\r\n\r\n\r\n\r\nNow, let’s take a look at what we created:\r\n\r\n\r\nints %>%\r\n  arrange(wpa)\r\n\r\n\r\n# A tibble: 403 x 16\r\n# Groups:   passer_player_name [65]\r\n   posteam passer_player_n~    wpa  week defteam     n team_name\r\n   <chr>   <chr>             <dbl> <int> <chr>   <int> <chr>    \r\n 1 ATL     M.Ryan           -0.534    14 LAC        11 Atlanta ~\r\n 2 LV      M.Mariota        -0.531    15 LAC         1 Las Vega~\r\n 3 LAC     J.Herbert        -0.489    14 ATL        10 Los Ange~\r\n 4 CIN     J.Burrow         -0.397     1 LAC         1 Cincinna~\r\n 5 SEA     R.Wilson         -0.335     7 ARI         4 Seattle ~\r\n 6 ATL     M.Ryan           -0.328     3 CHI         2 Atlanta ~\r\n 7 CIN     J.Burrow         -0.323     6 IND         4 Cincinna~\r\n 8 ARI     C.Streveler      -0.296    17 LA          1 Arizona ~\r\n 9 DAL     A.Dalton         -0.294    17 NYG         8 Dallas C~\r\n10 SEA     R.Wilson         -0.293    18 LA         14 Seattle ~\r\n# ... with 393 more rows, and 9 more variables: team_id <chr>,\r\n#   team_nick <chr>, team_color <chr>, team_color2 <chr>,\r\n#   team_color3 <chr>, team_color4 <chr>, team_logo_wikipedia <chr>,\r\n#   team_logo_espn <chr>, games_played <int>\r\n\r\nSo, since we sorted in descending order by wpa which stands for Win Probability Added, we see the interceptions that cost the most in terms of win probability at the top. The Joe Burrow interception from week 6 that we referenced earlier cost his team -32% win probability. If we dive deeper, and go back to the pbp dataframe, we can see that the Bengals win probability before the interception was 34.5%, and afterwards was just 2.2%.\r\n\r\n# A tibble: 1 x 6\r\n  posteam defteam passer_player_name  week away_wp away_wp_post\r\n  <chr>   <chr>   <chr>              <int>   <dbl>        <dbl>\r\n1 CIN     IND     J.Burrow               6   0.345       0.0221\r\n\r\nSo, we have already found which individual interceptions cost their team the most in win probability, but now let’s find which QBs have cost their teams the most cumulative win probability:\r\n\r\n\r\ncumulative_ints <- ints %>%\r\n  group_by(passer_player_name) %>%\r\n  summarize(posteam = posteam, n_int = max(n), cumulative_wpa = sum(wpa), team_color, team_color2, team_color3, .groups = \"keep\") %>%\r\n  filter(n_int > 5) %>%\r\n  slice(1)\r\ncumulative_ints %>%\r\n  arrange(cumulative_wpa)\r\n\r\n\r\n# A tibble: 32 x 7\r\n# Groups:   passer_player_name [32]\r\n   passer_player_n~ posteam n_int cumulative_wpa team_color\r\n   <chr>            <chr>   <int>          <dbl> <chr>     \r\n 1 M.Ryan           ATL        11          -1.71 #a71930   \r\n 2 R.Wilson         SEA        14          -1.65 #002244   \r\n 3 K.Murray         ARI        12          -1.53 #97233f   \r\n 4 C.Wentz          PHI        15          -1.52 #004953   \r\n 5 J.Goff           LA         13          -1.45 #002244   \r\n 6 N.Mullens        SF         12          -1.40 #aa0000   \r\n 7 K.Cousins        MIN        13          -1.34 #4f2683   \r\n 8 A.Smith          WAS         8          -1.32 #773141   \r\n 9 D.Jones          NYG        10          -1.31 #0b2265   \r\n10 B.Roethlisberger PIT        14          -1.24 #000000   \r\n# ... with 22 more rows, and 2 more variables: team_color2 <chr>,\r\n#   team_color3 <chr>\r\n\r\nIt looks like Falcons QB Matt Ryan has caused his team the most overall from his interceptions. This is impressive considering Ryan, with only 11 interceptions, has cost more total win probability than players with more interceptions, like Russell Wilson or Carson Wentz. This does not come as a surprise though, seeing as we found earlier that Ryan also has the most costly interception of the season, a week 14 interception against the Chargers that caused the Falcons to go from 84.4% to win, to just 31%.\r\nTo put the finishing touches on this analysis, let’s show a plot of this data we found that helps us more easily visualize where each QB stands. The following code uses ggplot2 to generate a plot, uses the colors data frame from earlier to make each QB’s point their team color, and filters any QB (or non-QB!) with 5 or fewer interceptions. The size of the dot represents the number of interceptions.\r\n\r\n\r\n# function for maintaining order in ggplot\r\ncumulative_ints$passer_player_name <- \r\n  factor(cumulative_ints$passer_player_name, levels = cumulative_ints$passer_player_name[order(cumulative_ints$cumulative_wpa)])\r\n\r\nggplot(data = cumulative_ints,\r\n       aes(x = passer_player_name, y = cumulative_wpa, size = n_int)) +\r\n  geom_point(color = cumulative_ints$team_color) +\r\n  theme_minimal() +\r\n  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = \"none\") +\r\n  labs(x = \"\", y = \"Cumulative Win Probability Added\", title = \"Which QB's Interceptions Cost Their Team the Most?\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-03-whose-interceptions-cost-their-team-the-most-tutorial/whose-interceptions-cost-their-team-the-most-tutorial_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-01-13T18:15:35-05:00",
    "input_file": "whose-interceptions-cost-their-team-the-most-tutorial.utf8.md"
  },
  {
    "path": "posts/2020-10-26-modeling-offensive-rebound-probability-using-spatial-analysis/",
    "title": "Modeling Offensive Rebound Probability using Spatial Analysis",
    "description": "Creating a model to predict the probability of an offensive rebound using NBA player tracking data.",
    "author": [
      {
        "name": "Ryan Flynn",
        "url": "https://ryanflynn.netlify.app/about.html"
      }
    ],
    "date": "2020-10-26",
    "categories": [],
    "contents": "\r\nIntroduction:\r\nRecently, I was lucky enough to work on a project given by the Oklahoma City Thunder as part of the hiring process for an analyst internship. I unfortunately did not get the position, but learned a lot, and would like to share my experience of the model I built.\r\nObjective\r\nThe objective of the project was to build a model, using artificial NBA player tracking data, that predicts the probability of an offensive rebound given the positions of the players on the court at two different time points on a given play: when the ball was shot, and when the ball hit the rim.\r\nApproach and Execution\r\nThe beginning of any project of this magnitude is to clean the data one is working with to ensure any errors and oddities are handled and to keep things organized. Using R, I decided to join together the play-by-play data with the location data and immediately filter out any made shots. Since the objective is to estimate probabilities of an offensive rebound, my model was trained based only on missed shots, in which a rebound occurred. Additionally, to deal with easier data manipulation, I shifted the points so that the rim, originally at (-41.75, 0), was at the origin (0,0). The big picture that I wanted to keep in mind throughout the project was that if I could predict where the ball was going to go based on where it was shot from, it would be much easier to predict who it was going to be rebounded by. In order to do this, I decided to divide the court into “zones” so I could analyze where the shooter was when the ball was shot, and where the rebounder was when the ball hit the rim. After a lot of thought on how to shape and size these zones, I decided to create the zones based on a polar coordinate system, so that they would be “wedge”-shaped, and equidistant from the rim from the left side of the court to the right side. The size of these zones was also important - I wanted to keep the zones small enough to get accurate, meaningful data on where the ball was most likely to go, but large enough that it was easily digestible and useful to a front office, coaching staff, or the players. It’s possible that more accurate, narrower estimates for ball locations could have been developed, but in my opinion, it was important to keep the zones large enough to strike a balance between precision and application. By making these zones using polar coordinates, I could plot each of the player coordinates and classify them into the zones they were located in.\r\nAfter the zones were created, I evaluated where rebounders were located at each different shot location. For example, in Figure 1 shown below, the black dots represent each time the ball was shot from the right wing (between 15 and 23.75 feet from the rim), and the red and blue dots represent where those shots were estimated. By doing this for each zone, I can generate rough estimates of probabilities of where an individual ball will land given that it was shot from a specific zone.\r\nFigure 1: Shooting Zone (black) vs. Rebounder Zone (red/blue)From here, I used a spatial analysis of Voronoi tessellations (Figure 2) to develop space “ownership” for each player around the court. In this figure, we can see the offensive players (blue) and defensive players (red) and their respective Voronoi tesselations. Each of these points represent the location of these players at the time the ball hit the rim, except for the shooter, whose location is the location at which he shot the ball. The polygons shown are the Voronoi tesselations, and represent the spatial ownership of each player. Each player owns 50% of the space between him and the other players around him, creating a boundary that represents the total space that we can realistically claim as their “own.” This decision of representing spatial ownership in this manner was under the assumption that since these are the “limits” to where the player can realistically get the ball, if the ball lands in a player’s Voronoi tesselation, they will be the one who rebounds it. This helped refine what before were probabilities that a ball landed in a specific zone into probabilities of a specific player getting the rebound. I then simply added the probabilities of each offensive player getting the rebound to develop a baseline cumulative offensive rebound probability. Using the play in Figure 2 as an example, the model projected that the offense had a 44.3% chance of securing a rebound, which is quite a bit higher than the average probability of an offensive rebound on any given play (~24%), This is likely due to the fact that since the shot was taken from the right corner, there is an increased likelihood that it lands on the left side of the court, and the offfense has the majority of the left side of the court “owned.” Additionally, shots from further out have an increased likelihood of resulting in a longer rebound, which would also mostly benefit the offense.\r\nFigure 2: Voronoi tesselations to represent spatial ownershipSo where exactly does this 44.3% prediction come from? Based on the data collected from the shots taken from this specific zone, which I coded as r4RC (ring 4, right corner, where ring 4 includes anything from 15 to 23.75 feet from the hoop), the most likely zone for the ball to land is r2LC (ring 2, left corner, where ring 2 includes anything from 3 to 8 feet from the hoop), where 26.7% of the shots landed. It can be seen from the Voronoi tessellations that the offensive player controls the majority of zone r2LC. Using Gauss’ formula for finding the area of irregular polygons, I found the area of each zone that is controlled by the players who make up that zone. In this example, the offensive player has control of 84.5% of zone r2LC, and 84.5% of 26.7% is awarded to that player. In other words, given the ball lands in zone r2LC (it has a 26.7% chance of doing so) then that offensive player has an 84.5% chance of securing the rebound. This is done for each player, in each zone, and all of their individual probabilities are added to formulate a single probability of an offensive rebound. Lastly, it’s important to note that this play originally had a 45.5% chance of being rebounded by the offense, but after adjusting for the rebounding ability of the players on the floor (where the defense had a slight advantage), it was bumped down to 44.3%.\r\nFinal Adjustments\r\nLastly, I wanted to incorporate player skill, but I wanted to be careful in doing so: Based on some exploratory data analysis along with my own basketball intuition, I believe that the likelihood of securing an offensive rebound is primarily based on the location of the player (and the location of the other players). However, it is important to inform these baseline predictions with the additional data we have about the particular players on the floor. To do this, I used the total “offensive rebounding rate” (offensive rebounds divided by offensive rebound chances) of each of the offensive players, and total defensive rebounding rate of the defensive players. I then developed Z-scores based on the mean and standard deviation of the rebounding rates of a lineup to establish if the offensive or defensive team had an “advantage” given how they fare compared to an average offensive or defensive lineup. I used an exponential formula to create a small, but meaningful bump or detriment to each team’s probability of offensive rebounding to give a final estimate of a team’s probability of securing an offensive rebound on a given play. If we return to the play from Figure 2, we can see that this play originally had a 45.5% chance of being rebounded by the offense, but after adjusting for the rebounding ability of the players on the floor (where the defense had a slight advantage), it was bumped down to 44.3%.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-26-modeling-offensive-rebound-probability-using-spatial-analysis/modeling-offensive-rebound-probability-using-spatial-analysis_files/shooting_zone_example.png",
    "last_modified": "2021-01-13T18:10:41-05:00",
    "input_file": "modeling-offensive-rebound-probability-using-spatial-analysis.utf8.md"
  },
  {
    "path": "posts/2020-08-10-glicko-2-modeling-of-mma-to-predict-win-probability/",
    "title": "Glicko-2 Modeling of MMA to Predict Win Probability",
    "description": "Using a variation of Elo, Glicko-2, to generate projected win probabilities and estimate true fighter rating.",
    "author": [
      {
        "name": "Ryan Flynn",
        "url": "https://ryanflynn.netlify.app/about.html"
      }
    ],
    "date": "2020-08-10",
    "categories": [],
    "contents": "\r\nIntroduction\r\nFor most of my life, my passion for sports has revolved around the same three sports: basketball, football, and baseball. In my mid-teens, I picked up a love for golf, both playing and watching, and in the past couple years, I have become enamored with mixed martial arts, specifically the UFC. This next project is my exploration of a fighter rating model that can evaluate the true rating of a fighter, and in turn, develop win probabilities for certain matchups.\r\nThe Data\r\nI have to give some big thanks to those who have contributed really helpful resources to help gather MMA data. Since MMA is still a relatively small sport, it’s much harder to gather data, especially non-professional data, on MMA fighters. However, a package called ufc.stats from Tamas Szilagyi scrapes data from the UFC site, and he does a great job of tidying the data into an easy-to-manage data frame in R. Lastly, I quickly realized that having just UFC data did not give me a large enough sample on most fighters. A user named Montanaz0r developed a great scraper of a site called Sherdog which contains every recorded fight of any fighter. This was extremely helpful and took my model to new heights.\r\nElo\r\nWhen starting the foundation of the model, I thought an Elo model would be appropriate, given the head-to-head nature of MMA. Elo is a rating system in which the pool of players (or in this case, fighters) maintain a constant average rating, which means after each match, the winning fighter has X amount of points added to their rating, and the loser has X amount of points subtracted from their rating. This number, X, is determined based on the relative win probabilities of the fighters. For example, if a fighter who has a 90% chance of winning wins, he/she won’t receive many points, since they were expected to win easily. However, that means the fighter who lost, but only had a 10% chance of winning, won’t receive a large deduction either, since they weren’t at all likely to win.\r\nAforementioned above, I quickkly realized I had an issue with sample size. MMA is a sport where the scheduling style presents a very unique issue relative to other major sports. In most sports, each team plays the same amount of times (at least in the regular season), against a set schedule, and there is often a lot of overlap in these schedules. In the UFC (or any professional MMA promotion), a fighter will only have the opportunity to fight more if they win a lot. For this reason, we see many, many fighters who have less than 5 total fights in the UFC, which makes it very difficult to find an estimate of their true rating. However, after some digging, I was able to pull all of a fighter’s matches, even from non-UFC bouts, which increased the average sample size tremendously. These non-UFC fights were given a slightly lesser weight, since the UFC is widely regarded as having the best talent of any MMA promotion. Since many of the fighters in other promotions are not UFC-caliber, I treated this similarly to how one may weigh priors of an NBA player’s college games vs. their professional games. Nonetheless, it gave me some prior information to build off of, which not only helped me find a more confident estimate of a fighter’s rating, but also fixed the problem of not having any estimate of a fighter’s rating if it was their UFC debut.\r\nTransition to Glicko-2\r\nAfter creating an iteration of this model that was based on this system, although it performed somewhat well, (RMSE = 0.15), there were some prevalent flaws that needed addressed. After doing some research, I found a system developed by Mark Glickman, a Statistics professor at Harvard University, called Glicko-2, which is a more complex variation of Elo. The main differentiating feature of Glicko-2 is it’s use of what is called a rating deviation (RD). A fighter’s RD, which starts at a constant value, decreases as we learn more about the fighter and converge on an estimate of their true rating. When we don’t have a large sample on a fighter, or when that sample is wildly inconsistent, and the RD is high, the win probability of a fight involving that fighter regresses to 50%.\r\nLet me give an example of why this is so helpful, and how it dramatically helped improve my predictions:\r\nJairzinho Rozenstruik, who had only 11 total fights under his belt (5 UFC fights) was set to fight Junior dos Santos on August 15th, who had 29 total fights (22 UFC fights). Rozenstruik, a rising prospect, was a slight favorite (~58% implied probability) according to Vegas odds, despite his disadvantage in experience compared to dos Santos. My original Elo model had Rozenstruik at 38% to win, almost entirely due to the fact that Rozenstruik having so few fights did not allow him the opportunity to grow his rating. However, when I deployed my Glicko-2 model, it had Rozenstruik at 54% to win, almost right in line with the implied probability. This surprised me. I figured that although the Glicko-2 model should decrease my confidence in my prediction of dos Santos winning, it shouldn’t change the direction, since all it is doing is using Rozenstruik’s high RD to regress to 50%, and shouldn’t take it over 50%. However, what I did not realize initially, was that the implementation of the RD has a snowball effect on the model. Since the points that are awarded to each fighter following a bout are based on their respective win probability, if the win probability projection is off, the point allocation is off as well. This means that in Rozenstruik’s past fights, he likely was not getting enough credit for the wins he was accruing, or dos Santos was not being punished enough for his losses. So now, although Rozenstruik still has a high RD, the estimate of his rating is much more accurate in the Glicko-2 model since the projections of his previous fights were also more accurate.\r\nLastly, I used BFGS optimization to determine my initial parameters for the model, and afterwards, the model has performed with an RMSE of about 0.08, almost half of what it was under the Elo model.\r\nThreats to Validity and Future Enhancements\r\nAlthough this Glicko-2 model fixed some of the flaws of the Elo model, there still can be many improvements. There are two major flaws that I am currently working on improving:\r\n1. All wins and losses are treated equally.\r\nIt goes without saying that not every win is the same. A first round knockout is certainly more impressive and more dominant than a 3-round split decision. Currently, the model does not take in differences in those outcomes. In the Elo model, I added an outcome adjustment that used a hierarchy of outcomes and assigned extra points to a fighter for the way in which they won, and took away extra points to a fighter for the way in which they lost. However, in the Glicko-2 model, this is much trickier, and although I briefly added a variation of this adjustment, it actually worsened my RMSE and log loss, so it has been removed until I am confident the adjustment is not adding noise to the model.\r\n2. All non-UFC bouts are treated equally.\r\nAlthough I have a lesser weight on non-UFC bouts, there are plenty of professional MMA promotions, and some of them have much better talent than others. Bellator, for example, is well renowned, and a win in Bellator is certainly more impressive than a win in a much lesser known, but still professional, MMA promotion. However, this adjustment seems trickier, and although giving different weights to different promotions may help improve the model, I do not want to assign some arbitrary weight to promotions without a data-driven reason behind it.\r\nShiny\r\nIf any are interested, I have also developed a Shiny web app where the predictions from this model can be seen for upcoming UFC fights at (https://rflynn.shinyapps.io/ufc_app/).\r\n\r\n\r\n\r\n",
    "preview": {},
<<<<<<< HEAD
    "last_modified": "2021-01-13T18:16:48-05:00",
    "input_file": "glicko-2-modeling-of-mma-to-predict-win-probability.utf8.md"
  },
  {
    "path": "posts/2020-05-04-creating-a-covid-19-dashboard-with-shiny/",
    "title": "Creating a COVID-19 Dashboard with Shiny",
    "description": "Using R Shiny to create an interactive web app for showing COVID-19 related metrics.",
    "author": [
      {
        "name": "Ryan Flynn",
        "url": "https://ryanflynn.netlify.app/about.html"
      }
    ],
    "date": "2020-05-04",
    "categories": [],
    "contents": "\r\nAs part of a course project, I used Shiny to make an interactive, comprehensive COVID-19 dashboard that allowed the user to clearly see metrics regarding the spread of the novel coronavirus. The web app is comprised of 4 separate parts, each of which I will go into detail on: a country-level plot, a state-level plot, a plot showing progression since stay-at-home orders were enacted by each state, and an ordinary differential equations (ODE) model that predicts the state-level progression of the virus from now (May 1st) until 50 days from now.\r\nInputs\r\nOne of the main features I wanted to implement in this dashboard was interactivity. I wanted the user to be able to see the progression of the virus on as many different levels as possible, while still maintaining clean, concise visualization that did not overshadow the actual metrics being demonstrated. To do this, the web app first allows the user to select a date on a slider, starting from March 1st. Next, the user can select a specific state to be shown in the tabs that are on the state level. Lastly, the user is able to select whether they want to view the plots in terms of new daily cases, or new daily deaths. Additionally, when a state is selected, any executive orders from that state are shown below the inputs, with the dates in which they were established.\r\n\r\nUS Plot\r\nThe default tab shows a choropleth of the United States where the states that have the highest numbers of new daily cases or deaths are shown in a deep red, whereas the states with the lowest numbers are represented in white. Additional user functionality is added by making the choropleth be interactive, where hovering the cursor over a specific state allows the user to see the exact number of new daily cases or deaths for that state. The total number of new cases or deaths is also shown in the top left corner of the plot.\r\n\r\nState Plot\r\nThe next tab, State Plot, allows the user to see a similar choropleth, this time of the state that they have chosen at the county level. The functionality and appearance of this tab is identical to the US plot, but serves as a way of seeing a more in-depth breakdown at what is shown in the first plot.\r\n\r\nDays Since Stay-at-Home Order\r\nThe next plot is a visual extension of the “Executive Orders” spot on the inputs tab, as it allows the user to see the progression of the disease relative to when a state passed a stay-at-home order. The state selected by the user is highlighted in red to more easily visualize that state in contrast to the other states. Additionally, “Day 0” on the plot is adjusted for if/when that state passed a stay-at-home order, which is why some states’ lines extend further than others. The user is also able to view this graph in terms of a 7-day average of new daily cases or new daily deaths, and these numbers are adjusted per 1 million capita.\r\n\r\nState Model\r\nLastly, the State Model tab shows a projection of how the next 50 days (from May 1st) will look in a state based on the state-specific parameters. The model uses a set of parameters estimated using ordinary differential equations (ODE). ODE based models have often been used for epidemiological estimation or even to evaluate changes in species concentration. This specific model was developed by Romik Ghosh.\r\nThe web app can be viewed at: (https://rflynn.shinyapps.io/covid-19/)\r\nNOTE:\r\nThe package used for this model was created for academic purposes, and has not been updated by the creator since May 4. All COVID-19 data beyond May 4 is not viewable in the web app.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-05-04-creating-a-covid-19-dashboard-with-shiny/stayhome.png",
    "last_modified": "2021-01-13T18:11:59-05:00",
    "input_file": "creating-a-covid-19-dashboard-with-shiny.utf8.md"
=======
    "last_modified": "2021-01-13T15:07:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-26-modeling-offensive-rebound-probability-using-spatial-analysis/",
    "title": "Modeling Offensive Rebound Probability using Spatial Analysis",
    "description": "Creating a model to predict the probability of an offensive rebound using NBA player tracking data.",
    "author": [
      {
        "name": "Ryan Flynn",
        "url": "ryanflynn.netlify.app"
      }
    ],
    "date": "2020-10-26",
    "categories": [],
    "contents": "\r\nIntroduction:\r\nRecently, I was lucky enough to work on a project given by the Oklahoma City Thunder as part of the hiring process for an analyst internship. I unfortunately did not get the position, but learned a lot, and would like to share my experience of the model I built.\r\nObjective\r\nThe objective of the project was to build a model, using artificial NBA player tracking data, that predicts the probability of an offensive rebound given the positions of the players on the court at two different time points on a given play: when the ball was shot, and when the ball hit the rim.\r\nApproach and Execution\r\nThe beginning of any project of this magnitude is to clean the data one is working with to ensure any errors and oddities are handled and to keep things organized. Using R, I decided to join together the play-by-play data with the location data and immediately filter out any made shots. Since the objective is to estimate probabilities of an offensive rebound, my model was trained based only on missed shots, in which a rebound occurred. Additionally, to deal with easier data manipulation, I shifted the points so that the rim, originally at (-41.75, 0), was at the origin (0,0). The big picture that I wanted to keep in mind throughout the project was that if I could predict where the ball was going to go based on where it was shot from, it would be much easier to predict who it was going to be rebounded by. In order to do this, I decided to divide the court into “zones” so I could analyze where the shooter was when the ball was shot, and where the rebounder was when the ball hit the rim. After a lot of thought on how to shape and size these zones, I decided to create the zones based on a polar coordinate system, so that they would be “wedge”-shaped, and equidistant from the rim from the left side of the court to the right side. The size of these zones was also important - I wanted to keep the zones small enough to get accurate, meaningful data on where the ball was most likely to go, but large enough that it was easily digestible and useful to a front office, coaching staff, or the players. It’s possible that more accurate, narrower estimates for ball locations could have been developed, but in my opinion, it was important to keep the zones large enough to strike a balance between precision and application. By making these zones using polar coordinates, I could plot each of the player coordinates and classify them into the zones they were located in.\r\nAfter the zones were created, I evaluated where rebounders were located at each different shot location. For example, in Figure 1 shown below, the black dots represent each time the ball was shot from the right wing (between 15 and 23.75 feet from the rim), and the red and blue dots represent where those shots were estimated. By doing this for each zone, I can generate rough estimates of probabilities of where an individual ball will land given that it was shot from a specific zone.\r\nFigure 1: Shooting Zone (black) vs. Rebounder Zone (red/blue)From here, I used a spatial analysis of Voronoi tessellations (Figure 2) to develop space “ownership” for each player around the court. In this figure, we can see the offensive players (blue) and defensive players (red) and their respective Voronoi tesselations. Each of these points represent the location of these players at the time the ball hit the rim, except for the shooter, whose location is the location at which he shot the ball. The polygons shown are the Voronoi tesselations, and represent the spatial ownership of each player. Each player owns 50% of the space between him and the other players around him, creating a boundary that represents the total space that we can realistically claim as their “own.” This decision of representing spatial ownership in this manner was under the assumption that since these are the “limits” to where the player can realistically get the ball, if the ball lands in a player’s Voronoi tesselation, they will be the one who rebounds it. This helped refine what before were probabilities that a ball landed in a specific zone into probabilities of a specific player getting the rebound. I then simply added the probabilities of each offensive player getting the rebound to develop a baseline cumulative offensive rebound probability. Using the play in Figure 2 as an example, the model projected that the offense had a 44.3% chance of securing a rebound, which is quite a bit higher than the average probability of an offensive rebound on any given play (~24%), This is likely due to the fact that since the shot was taken from the right corner, there is an increased likelihood that it lands on the left side of the court, and the offfense has the majority of the left side of the court “owned.” Additionally, shots from further out have an increased likelihood of resulting in a longer rebound, which would also mostly benefit the offense.\r\nFigure 2: Voronoi tesselations to represent spatial ownershipSo where exactly does this 44.3% prediction come from? Based on the data collected from the shots taken from this specific zone, which I coded as r4RC (ring 4, right corner, where ring 4 includes anything from 15 to 23.75 feet from the hoop), the most likely zone for the ball to land is r2LC (ring 2, left corner, where ring 2 includes anything from 3 to 8 feet from the hoop), where 26.7% of the shots landed. It can be seen from the Voronoi tessellations that the offensive player controls the majority of zone r2LC. Using Gauss’ formula for finding the area of irregular polygons, I found the area of each zone that is controlled by the players who make up that zone. In this example, the offensive player has control of 84.5% of zone r2LC, and 84.5% of 26.7% is awarded to that player. In other words, given the ball lands in zone r2LC (it has a 26.7% chance of doing so) then that offensive player has an 84.5% chance of securing the rebound. This is done for each player, in each zone, and all of their individual probabilities are added to formulate a single probability of an offensive rebound. Lastly, it’s important to note that this play originally had a 45.5% chance of being rebounded by the offense, but after adjusting for the rebounding ability of the players on the floor (where the defense had a slight advantage), it was bumped down to 44.3%.\r\nFinal Adjustments\r\nLastly, I wanted to incorporate player skill, but I wanted to be careful in doing so: Based on some exploratory data analysis along with my own basketball intuition, I believe that the likelihood of securing an offensive rebound is primarily based on the location of the player (and the location of the other players). However, it is important to inform these baseline predictions with the additional data we have about the particular players on the floor. To do this, I used the total “offensive rebounding rate” (offensive rebounds divided by offensive rebound chances) of each of the offensive players, and total defensive rebounding rate of the defensive players. I then developed Z-scores based on the mean and standard deviation of the rebounding rates of a lineup to establish if the offensive or defensive team had an “advantage” given how they fare compared to an average offensive or defensive lineup. I used an exponential formula to create a small, but meaningful bump or detriment to each team’s probability of offensive rebounding to give a final estimate of a team’s probability of securing an offensive rebound on a given play. If we return to the play from Figure 2, we can see that this play originally had a 45.5% chance of being rebounded by the offense, but after adjusting for the rebounding ability of the players on the floor (where the defense had a slight advantage), it was bumped down to 44.3%.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-13T15:51:26-05:00",
    "input_file": {}
>>>>>>> bb05995bbf72a7eb2ca4149b06b7c2b49eb7bf9d
  }
]
